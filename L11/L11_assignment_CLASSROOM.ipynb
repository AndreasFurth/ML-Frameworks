{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7542e876",
      "metadata": {},
      "source": [
        "# Lektion 11 - Prestandaoptimering och fine-tuning\n",
        "\n",
        "**Assignment: Transfer learning and data pipeline tuning**\n",
        "\n",
        "Instructions:\n",
        "1. Use a pretrained model (e.g., ResNet18)\n",
        "2. Compare frozen vs fine-tuned performance\n",
        "3. Measure small performance tweaks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29d06b2c",
      "metadata": {},
      "source": [
        "## Task 1: Transfer learning\n",
        "Start with a pretrained model and a new classifier head."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "7c1955b2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Load a pretrained model\n",
        "import torch\n",
        "from torchvision import models\n",
        "\n",
        "# Vi hämtar hem resnet mha torchsvision, och hämtar vikterna från varianten\n",
        "# som är tränad på ImageNets\n",
        "model = models.resnet18(weights = models.ResNet18_Weights.IMAGENET1K_V1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "b7cac0af",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Vi tar en span på modellen\n",
        "\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "6fd0544e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Freeze the base layers\n",
        "from torch.nn import Linear\n",
        "\n",
        "num_feats = model.fc.in_features\n",
        "model.fc = Linear(num_feats, 10)\n",
        "\n",
        "# Här sätter vi gradient till false, alltså\n",
        "# vikterna kommer inte uppdateras vid backprop\n",
        "# Vi börjar med att freeza alla lager\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Sedan tinar vi vårt fully connected layer på slutet\n",
        "for param in model.fc.parameters():\n",
        "    param.requires_grad = True\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "5f9f5096",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Move the model to a gpu device\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        "\n",
        "model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "162e7c29",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# Vi behöver ett dataset att träna modellen på! \n",
        "# Vi kör CIFAR-10\n",
        "\n",
        "from torchvision import datasets\n",
        "# The downloaded data is in the form of PIL images, we need to transform them to tensors\n",
        "from torchvision import transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "\n",
        "train_data = datasets.CIFAR10(root = \"data\", train = True, download = True, transform = transform)\n",
        "test_data = datasets.CIFAR10(root = \"data\", train = False, download = True, transform = transform)\n",
        "\n",
        "\n",
        "# Vi behöver också en dataloader\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Shuffle = True för träning, False för testning för att bibehålla ordningen på testdata\n",
        "train_dataloader = DataLoader(train_data, batch_size = 32, shuffle = True)\n",
        "test_dataloader = DataLoader(test_data, batch_size = 32, shuffle = False)   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "f5691baa",
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Train a new classifier head\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "# Istället för att stoppa in model.parameters() \n",
        "# Så stoppar vi nu bara in model.fc.parameters()\n",
        "# Alltså parametrarna i vårt Fully connected layer\n",
        "optimizer = optim.Adam(model.fc.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Träningsloop\n",
        "num_epochs = 1\n",
        "for _ in range(num_epochs):\n",
        "    model.train()\n",
        "    for xb, yb in train_dataloader:\n",
        "        # Vi ser till att flytta vår batchdata till devicen!\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(xb)\n",
        "        loss = criterion(logits, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "c0b6d5ce",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Accuracy: 0.45\n",
            "Test Accuracy: 0.44\n"
          ]
        }
      ],
      "source": [
        "# TODO: Record accuracy\n",
        "\n",
        "def eval_acc(model, loader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            preds = torch.argmax(model(xb), dim=1)\n",
        "            correct += (preds == yb).sum().item()\n",
        "            total += yb.size(0)\n",
        "    return correct / max(total, 1)\n",
        "\n",
        "train_acc = eval_acc(model, train_dataloader)\n",
        "test_acc = eval_acc(model, test_dataloader)\n",
        "\n",
        "print(f\"Train Accuracy: {train_acc:.2f}\")\n",
        "print(f\"Test Accuracy: {test_acc:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "298e14d1",
      "metadata": {},
      "source": [
        "## Task 2: Fine-tuning\n",
        "Unfreeze part of the base and compare performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "28f0e971",
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Unfreeze part of the base\n",
        "\n",
        "# Nu, så unfreezar vi en del av basen, t.ex layer4\n",
        "for name, param in model.named_parameters():\n",
        "    if name.startswith(\"layer4\"):\n",
        "        param.requires_grad = True\n",
        "\n",
        "# I uttrycket nedan gör vi lite pythonhax för att sätta alla lager\n",
        "# där parametrarna.requires_grad = True, och ger de en superlåg learning rate\n",
        "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.0005)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "a87e2f26",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fine-tuned accuracy: 0.6720\n"
          ]
        }
      ],
      "source": [
        "# TODO: Train again and record accuracy\n",
        "\n",
        "num_epochs = 1\n",
        "for _ in range(num_epochs):\n",
        "    model.train()\n",
        "    for xb, yb in train_dataloader:\n",
        "        # Vi ser till att flytta vår batchdata till devicen!\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(xb)\n",
        "        loss = criterion(logits, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "acc_tuned = eval_acc(model, test_dataloader)\n",
        "print(f\"Fine-tuned accuracy: {acc_tuned:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "dba3b59a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Accuracy: 0.73\n",
            "Test Accuracy: 0.67\n"
          ]
        }
      ],
      "source": [
        "# TODO: Compare with Task 1\n",
        "\n",
        "# TODO: Record accuracy\n",
        "\n",
        "def eval_acc(model, loader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            preds = torch.argmax(model(xb), dim=1)\n",
        "            correct += (preds == yb).sum().item()\n",
        "            total += yb.size(0)\n",
        "    return correct / max(total, 1)\n",
        "\n",
        "train_acc = eval_acc(model, train_dataloader)\n",
        "test_acc = eval_acc(model, test_dataloader)\n",
        "\n",
        "print(f\"Train Accuracy: {train_acc:.2f}\")\n",
        "print(f\"Test Accuracy: {test_acc:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1722d431",
      "metadata": {},
      "source": [
        "## Task 3: Dataloader tuning\n",
        "Measure the effect of data loader settings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5eed4cef",
      "metadata": {},
      "outputs": [],
      "source": [
        "# FUN FACTS!\n",
        "\n",
        "#  TODO: Test at least two of:\n",
        "# - num_workers (bra på linux, ok på windows, kass på mac)\n",
        "# - pin_memory (bra på linux och windows, kass på mac)\n",
        "# - batch size (gör så stor som möjligt, börja på 32)\n",
        "\n",
        "\n",
        "# Olika datorer har olika hårdvara och mjukvara\n",
        "# Vi vill i regel distribuera datorkraft så optimalt som möjligt\n",
        "# För vår ML-modell. \n",
        "\n",
        "# num_workers är ett sätt att dela upp hur många enheter som gör beräkning samtidigt\n",
        "# MEN: på Windows och Mac är instantieriengen av Spawn-typ (den skapar en helt ny process, tar lång tid)\n",
        "# På linux är det Fork-typ, en direkt kopia, mer eller mindre direkt\n",
        "# Det är värt att hålla koll på iaf. \n",
        "\n",
        "# Batch size är hur mycket data som matas in i modellen åt gången. \n",
        "# Batch size är också något som kan optimera tidseffektivitet och minneshantering under träning\n",
        "# Vi vill typiskt ha så stor batch som möjligt\n",
        "# MEN: för stor batch size kan göra att datorn får slut på minne\n",
        "# Så vi börjar typiskt med 32, och kan sedan stega upp i 2-potenser (32, 64, 128, 256, 512, ...)\n",
        "\n",
        "# Olika typer av datorer har olika minneshantering. \n",
        "# På Mac är det Unified RAM, \n",
        "# På windows Fixed VRAM\n",
        "# På linux Fixed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "9930727a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Record training time for 1-2 epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "aaf9ae06",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done! You explored fine-tuning and performance tuning.\n"
          ]
        }
      ],
      "source": [
        "print(\"Done! You explored fine-tuning and performance tuning.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ml-frameworks",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
