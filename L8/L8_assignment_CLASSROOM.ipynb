{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8d94ae7e",
      "metadata": {},
      "source": [
        "# Lektion 8 - ML pipelines: Struktur och automatisering\n",
        "\n",
        "**Assignment: Build a minimal training pipeline**\n",
        "\n",
        "Instructions:\n",
        "1. Create a small end-to-end pipeline in code\n",
        "2. Save outputs and metrics\n",
        "3. Keep short comments explaining design choices"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9aca02f0",
      "metadata": {},
      "source": [
        "## Task 1: Pipeline in code\n",
        "Build a small end-to-end pipeline with preprocessing and a model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "968b3d95",
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Build a scikit-learn Pipeline with:\n",
        "# - StandardScaler\n",
        "# - Model of choice (LogisticRegression or SVC)\n",
        "\n",
        "# En pipeline är en serie av steg som vi kör\n",
        "# Inom ML, använder vi ofta pipelines för preprocessingsteg\n",
        "# som till exempel standardisering, transformering och reshaping, \n",
        "# men även för träning.\n",
        "# \n",
        "# Idag bygger vi en pipeline med standardisering och modellskapande\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Nedan bygger vi en pipeline.\n",
        "# Pipelinen som vi har byggt sätter ihop både vårt preprocessingsteg\n",
        "# och skapandet av vår modell till en körning.\n",
        "# Det blir då väldigt enkelt att återskapa samma flöde\n",
        "pipeline = Pipeline(\n",
        "    steps = [\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"model\", LogisticRegression(max_iter=1000))\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "eec2ac11",
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Train and evaluate on a dataset\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "data = load_iris(as_frame=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.25, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "91ac6091",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_test shape: (38, 4)\n",
            "X_train shape: (112, 4)\n"
          ]
        }
      ],
      "source": [
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"X_train shape:\", X_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6277531b",
      "metadata": {},
      "source": [
        "# Se L4_assignment_CLASSROOM.ipynb för utförlig EDA:\n",
        "\n",
        "[L4_assignment_CLASSROOM.ipynb](../L4/L4_assignment_CLASSROOM.ipynb)\n",
        "\n",
        "[Web version](https://github.com/AndreasFurth/ML-Frameworks/blob/main/L4/L4_assignment_CLASSROOM.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "0d254d03",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'accuracy': 1.0, 'f1_macro': 1.0}\n"
          ]
        }
      ],
      "source": [
        "# The training:\n",
        "\n",
        "\n",
        "# Med vår pipeline så tränar vi, och sedan utvärderar vår modell\n",
        "\n",
        "pipeline.fit(X_train,y_train)\n",
        "preds = pipeline.predict(X_test)\n",
        "\n",
        "metrics = {\n",
        "    \"accuracy\": accuracy_score(y_test, preds),\n",
        "    \"f1_macro\": f1_score(y_test, preds, average=\"macro\")\n",
        "}\n",
        "\n",
        "print(metrics)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb6b14e3",
      "metadata": {},
      "source": [
        "## Task 2: Automate training\n",
        "Wrap the workflow into a reusable experiment function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3084d35",
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Wrap training in a function run_experiment(config)\n",
        "\n",
        "# En sån här funktion riskerar att bli väldigt lång\n",
        "# Vi måste gå balansgång mellan generaliserbarhet \n",
        "# och korthet/läsbarhet.\n",
        "# Om idéen är att kunna återanvända vår experimentfunktion\n",
        "# så är lång längd ett piller som vi kan behöva svälja\n",
        "def run_experiment(config = {\"scaler\": StandardScaler(), \"model\": LogisticRegression(max_iter=1000), \"params\": None}):\n",
        "    \n",
        "    # Nedan är en generaliserbar pipeline\n",
        "    # MEN, den är kanske inte maximalt användbar,\n",
        "    # eftersom användaren tvingas hålla koll på \n",
        "    # och skicka in alla separata steg själv\n",
        "    pipeline = Pipeline(\n",
        "        steps= [\n",
        "            (\"scaler\", config[\"scaler\"]),\n",
        "            (\"model\", config[\"model\"]),\n",
        "            (\"params\", config[\"params\"])\n",
        "        ]\n",
        "    )\n",
        "\n",
        "def run_experiment_not_generalised(config):\n",
        "    # Use a small dataset to keep runtime low\n",
        "    iris = load_iris()\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        iris.data, iris.target, test_size=0.2, random_state=42, stratify=iris.target\n",
        "    )\n",
        "\n",
        "    pipeline = Pipeline(\n",
        "        steps=[\n",
        "            (\"scaler\", StandardScaler()),\n",
        "            (\"model\", LogisticRegression(max_iter=config[\"max_iter\"], C=config[\"C\"])),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    pipeline.fit(X_train, y_train)\n",
        "    preds = pipeline.predict(X_test)\n",
        "\n",
        "    metrics = {\n",
        "        \"accuracy\": accuracy_score(y_test, preds),\n",
        "        \"f1_macro\": f1_score(y_test, preds, average=\"macro\"),\n",
        "        \"params\": {\"C\": config[\"C\"], \"max_iter\": config[\"max_iter\"]},\n",
        "    }\n",
        "\n",
        "    # Save outputs\n",
        "    (\"metrics.json\").write_text(json.dumps(metrics, indent=2))\n",
        "    joblib.dump(pipeline, \"model.joblib\")\n",
        "\n",
        "    return metrics\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "c6cb0010",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import get_scorer\n",
        "\n",
        "def run_experiment(config):\n",
        "    \"\"\"\n",
        "    Run a general ML experiment entirely from a config dict.\n",
        "\n",
        "    config: dict\n",
        "        {\n",
        "            \"X\": feature matrix,\n",
        "            \"y\": target vector,\n",
        "            \"test_size\": float (optional, default=0.2),\n",
        "            \"random_state\": int (optional, default=42),\n",
        "            \"preprocessing\": list of (name, transformer) tuples (optional),\n",
        "            \"model\": sklearn estimator,\n",
        "            \"params\": dict of hyperparameters for GridSearchCV (optional),\n",
        "            \"scoring\": str or callable metric (optional, default='accuracy')\n",
        "        }\n",
        "    \"\"\"\n",
        "    \n",
        "    # Extract from config with defaults\n",
        "    # Vi hämtar de relevanta värdena från config\n",
        "    X = config[\"X\"]\n",
        "    y = config[\"y\"]\n",
        "    test_size = config.get(\"test_size\", 0.2)\n",
        "    random_state = config.get(\"random_state\", 42)\n",
        "    preprocessing = config.get(\"preprocessing\", [])\n",
        "    model = config[\"model\"]\n",
        "    params = config.get(\"params\", None)\n",
        "    scoring = config.get(\"scoring\", \"accuracy\")\n",
        "    \n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=test_size, random_state=random_state\n",
        "    )\n",
        "    \n",
        "    # Build pipeline\n",
        "    # Nedan så tar bygger i en lista av steps\n",
        "    # först är all preprocessing (scaling, transforms etc)\n",
        "    # och sist är modellen.\n",
        "    steps = preprocessing + [(\"model\", model)] \n",
        "\n",
        "    # steps ser typiskt ut ungefär såhär\n",
        "#    steps= [\n",
        "#             (\"scaler\", config[\"scaler\"]),\n",
        "#             (\"model\", config[\"model\"]), \n",
        "#        ]\n",
        "\n",
        "    pipeline = Pipeline(steps=steps)\n",
        "    \n",
        "    # Wrap with GridSearchCV if params provided\n",
        "    # ÖVERKURS: kika själva om ni är nyfikna\n",
        "    if params:\n",
        "        pipeline = GridSearchCV(pipeline, params, cv=5, n_jobs=-1, scoring=scoring)\n",
        "    \n",
        "    # Fit\n",
        "    pipeline.fit(X_train, y_train)\n",
        "    \n",
        "    # Evaluate\n",
        "    # Här använder vi sklearns get_scorer() för att räkna ut vår performance\n",
        "    # tidigare har vi i regel importerat en specifik metric (f1-score, accuracy)\n",
        "    # Vi brukar också köra confusion matrix, det kan vi göra senare\n",
        "    scorer = get_scorer(scoring)\n",
        "    score = scorer(pipeline, X_test, y_test)\n",
        "    \n",
        "    print(f\"{scoring} on test set: {score:.4f}\")\n",
        "    \n",
        "    return pipeline, score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "a1759fd8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy on test set: 0.8421\n",
            "accuracy on test set: 0.9778\n",
            "accuracy on test set: 0.9667\n"
          ]
        }
      ],
      "source": [
        "# TODO: Save metrics to metrics.json\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.svm import SVC\n",
        "import joblib\n",
        "import json\n",
        "\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "configs = []\n",
        "config_LogReg_025 = {\n",
        "    \"config_name\": \"LogReg_025\",\n",
        "    \"X\": X,\n",
        "    \"y\": y,\n",
        "    \"test_size\": 0.25,\n",
        "    \"random_state\": 123,\n",
        "    \"preprocessing\": [\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"pca\", PCA(n_components=2))\n",
        "    ],\n",
        "    \"model\": LogisticRegression(max_iter=1000),\n",
        "    \"params\": {\n",
        "        \"model__C\": [0.1, 1, 10],\n",
        "    },\n",
        "    \"scoring\": \"accuracy\"\n",
        "}\n",
        "\n",
        "config_SVM_03 = {\n",
        "    \"config_name\": \"SVM_03\",\n",
        "    \"X\": X,\n",
        "    \"y\": y,\n",
        "    \"test_size\": 0.3,\n",
        "    \"random_state\": 456,\n",
        "    \"preprocessing\": [\n",
        "        (\"scaler\", StandardScaler())\n",
        "    ],\n",
        "    \"model\": SVC(),\n",
        "    \"params\": {\n",
        "        \"model__C\": [0.1, 1, 10],\n",
        "        \"model__kernel\": [\"linear\", \"rbf\"]\n",
        "    },\n",
        "    \"scoring\": \"accuracy\"\n",
        "}\n",
        "\n",
        "# create a config for a third experiment with different preprocessing and model\n",
        "config_LogReg_02 = {\n",
        "    \"config_name\": \"LogReg_02\",\n",
        "    \"X\": X,\n",
        "    \"y\": y,\n",
        "    \"test_size\": 0.2,\n",
        "    \"random_state\": 789,\n",
        "    \"preprocessing\": [\n",
        "        (\"scaler\", StandardScaler())\n",
        "    ],\n",
        "    \"model\": LogisticRegression(max_iter=1000),\n",
        "    \"params\": {\n",
        "        \"model__C\": [0.01, 0.1, 1],\n",
        "    },\n",
        "    \"scoring\": \"accuracy\"\n",
        "}\n",
        "\n",
        "configs = [config_LogReg_025, config_SVM_03, config_LogReg_02]\n",
        "\n",
        "for config in configs:\n",
        "\n",
        "    pipeline, score = run_experiment(config)\n",
        "\n",
        "    # Save outputs\n",
        "    metrics = {\n",
        "        \"accuracy\": score,\n",
        "        \"params\": pipeline.best_params_ if hasattr(pipeline, \"best_params_\") else None\n",
        "    }\n",
        "\n",
        "    # save our metrics to a json file\n",
        "    with open(f\"metrics_{config['config_name']}.json\", \"w\") as f:\n",
        "        json.dump(metrics, f, indent=2) \n",
        "\n",
        "    # save our model to a joblib file\n",
        "    joblib.dump(pipeline, f\"model_{config['config_name']}.joblib\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "cc2abe4f",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['model.joblib']"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# TODO: Save the trained model with joblib\n",
        "\n",
        "joblib.dump(pipeline, \"model.joblib\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8870b9b7",
      "metadata": {},
      "source": [
        "## Task 3: CLI (optional but recommended)\n",
        "Parameterize runs and log chosen values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35f40e1c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Use argparse to pass model params (e.g., C, max_iter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5358f887",
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Log chosen params into metrics.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "454e3c82",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Done! You created a small reproducible ML pipeline.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ml-frameworks",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
