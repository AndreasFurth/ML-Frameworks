{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8d94ae7e",
      "metadata": {},
      "source": [
        "# Lektion 8 - ML pipelines: Struktur och automatisering\n",
        "\n",
        "**Assignment: Build a minimal training pipeline**\n",
        "\n",
        "Instructions:\n",
        "1. Create a small end-to-end pipeline in code\n",
        "2. Save outputs and metrics\n",
        "3. Keep short comments explaining design choices"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9aca02f0",
      "metadata": {},
      "source": [
        "## Task 1: Pipeline in code\n",
        "Build a small end-to-end pipeline with preprocessing and a model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "968b3d95",
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Build a scikit-learn Pipeline with:\n",
        "# - StandardScaler\n",
        "# - Model of choice (LogisticRegression or SVC)\n",
        "\n",
        "# En pipeline är en serie av steg som vi kör\n",
        "# Inom ML, använder vi ofta pipelines för preprocessingsteg\n",
        "# som till exempel standardisering, transformering och reshaping, \n",
        "# men även för träning.\n",
        "# \n",
        "# Idag bygger vi en pipeline med standardisering och modellskapande\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Nedan bygger vi en pipeline.\n",
        "# Pipelinen som vi har byggt sätter ihop både vårt preprocessingsteg\n",
        "# och skapandet av vår modell till en körning.\n",
        "# Det blir då väldigt enkelt att återskapa samma flöde\n",
        "pipeline = Pipeline(\n",
        "    steps = [\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"model\", LogisticRegression(max_iter=1000))\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "eec2ac11",
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Train and evaluate on a dataset\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "data = load_iris(as_frame=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.25, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "91ac6091",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_test shape: (38, 4)\n",
            "X_train shape: (112, 4)\n"
          ]
        }
      ],
      "source": [
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"X_train shape:\", X_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6277531b",
      "metadata": {},
      "source": [
        "# Se L4_assignment_CLASSROOM.ipynb för utförlig EDA:\n",
        "\n",
        "[L4_assignment_CLASSROOM.ipynb](../L4/L4_assignment_CLASSROOM.ipynb)\n",
        "\n",
        "[Web version](https://github.com/AndreasFurth/ML-Frameworks/blob/main/L4/L4_assignment_CLASSROOM.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d254d03",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'accuracy': 1.0, 'f1_macro': 1.0}\n"
          ]
        }
      ],
      "source": [
        "# The training:\n",
        "\n",
        "\n",
        "# Med vår pipeline så tränar vi, och sedan utvärderar vår modell\n",
        "\n",
        "pipeline.fit(X_train,y_train)\n",
        "preds = pipeline.predict(X_test)\n",
        "\n",
        "metrics = {\n",
        "    \"accuracy\": accuracy_score(y_test, preds),\n",
        "    \"f1_macro\": f1_score(y_test, preds, average=\"macro\")\n",
        "}\n",
        "\n",
        "print(metrics)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb6b14e3",
      "metadata": {},
      "source": [
        "## Task 2: Automate training\n",
        "Wrap the workflow into a reusable experiment function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3084d35",
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Wrap training in a function run_experiment(config)\n",
        "\n",
        "# En sån här funktion riskerar att bli väldigt lång\n",
        "# Vi måste gå balansgång mellan generaliserbarhet \n",
        "# och korthet/läsbarhet.\n",
        "# Om idéen är att kunna återanvända vår experimentfunktion\n",
        "# så är lång längd ett piller som vi kan behöva svälja\n",
        "def run_experiment(config = {\"scaler\": StandardScaler(), \"model\": LogisticRegression(max_iter=1000), \"params\": None}):\n",
        "    \n",
        "    # Nedan är en generaliserbar pipeline\n",
        "    # MEN, den är kanske inte maximalt användbar,\n",
        "    # eftersom användaren tvingas hålla koll på \n",
        "    # och skicka in alla separata steg själv\n",
        "    pipeline = Pipeline(\n",
        "        steps= [\n",
        "            (\"scaler\", config[\"scaler\"]),\n",
        "            (\"model\", config[\"model\"]),\n",
        "            (\"params\", config[\"params\"])\n",
        "        ]\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6cb0010",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import get_scorer\n",
        "\n",
        "def run_experiment(config):\n",
        "    \"\"\"\n",
        "    Run a general ML experiment entirely from a config dict.\n",
        "\n",
        "    config: dict\n",
        "        {\n",
        "            \"X\": feature matrix,\n",
        "            \"y\": target vector,\n",
        "            \"test_size\": float (optional, default=0.2),\n",
        "            \"random_state\": int (optional, default=42),\n",
        "            \"preprocessing\": list of (name, transformer) tuples (optional),\n",
        "            \"model\": sklearn estimator,\n",
        "            \"params\": dict of hyperparameters for GridSearchCV (optional),\n",
        "            \"scoring\": str or callable metric (optional, default='accuracy')\n",
        "        }\n",
        "    \"\"\"\n",
        "    \n",
        "    # Extract from config with defaults\n",
        "    X = config[\"X\"]\n",
        "    y = config[\"y\"]\n",
        "    test_size = config.get(\"test_size\", 0.2)\n",
        "    random_state = config.get(\"random_state\", 42)\n",
        "    preprocessing = config.get(\"preprocessing\", [])\n",
        "    model = config[\"model\"]\n",
        "    params = config.get(\"params\", None)\n",
        "    scoring = config.get(\"scoring\", \"accuracy\")\n",
        "    \n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=test_size, random_state=random_state\n",
        "    )\n",
        "    \n",
        "    # Build pipeline\n",
        "    steps = preprocessing + [(\"model\", model)]\n",
        "    pipeline = Pipeline(steps=steps)\n",
        "    \n",
        "    # Wrap with GridSearchCV if params provided\n",
        "    if params:\n",
        "        pipeline = GridSearchCV(pipeline, params, cv=5, n_jobs=-1, scoring=scoring)\n",
        "    \n",
        "    # Fit\n",
        "    pipeline.fit(X_train, y_train)\n",
        "    \n",
        "    # Evaluate\n",
        "    scorer = get_scorer(scoring)\n",
        "    score = scorer(pipeline, X_test, y_test)\n",
        "    \n",
        "    print(f\"{scoring} on test set: {score:.4f}\")\n",
        "    \n",
        "    return pipeline, score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1759fd8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Save metrics to metrics.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc2abe4f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Save the trained model with joblib"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8870b9b7",
      "metadata": {},
      "source": [
        "## Task 3: CLI (optional but recommended)\n",
        "Parameterize runs and log chosen values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35f40e1c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Use argparse to pass model params (e.g., C, max_iter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5358f887",
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Log chosen params into metrics.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "454e3c82",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Done! You created a small reproducible ML pipeline.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ml-frameworks",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
